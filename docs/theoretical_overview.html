<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Gaussian Assumption for Normality &mdash; EDA Toolkit 0.0.13 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css" />
      <link rel="stylesheet" type="text/css" href="_static/custom.js" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/custom.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Data Management Overview" href="data_management.html" />
    <link rel="prev" title="Welcome to the EDA Toolkit Python Library Documentation!" href="getting_started.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            EDA Toolkit
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Welcome to the EDA Toolkit Python Library Documentation!</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html#description">Description</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Theoretical Overview</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Gaussian Assumption for Normality</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#histograms-and-kernel-density-estimation-kde">Histograms and Kernel Density Estimation (KDE)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#pearson-correlation-coefficient">Pearson Correlation Coefficient</a></li>
<li class="toctree-l1"><a class="reference internal" href="#box-cox-transformation">Box-Cox Transformation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#mathematical-definition">Mathematical Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="#properties-and-benefits">Properties and Benefits</a></li>
<li class="toctree-l2"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#applications-in-modeling">Applications in Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#confidence-intervals-for-lambda">Confidence Intervals for Lambda</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#the-yeo-johnson-transformation">The Yeo-Johnson Transformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="#median-and-iqr-scaling">Median and IQR Scaling</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#centering-data-using-the-median">Centering Data Using the Median</a></li>
<li class="toctree-l2"><a class="reference internal" href="#explanation-of-each-component">Explanation of Each Component</a></li>
<li class="toctree-l2"><a class="reference internal" href="#example-calculation">Example Calculation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#logit-transformation">Logit Transformation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#purpose-and-assumptions">Purpose and Assumptions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#partial-dependence-foundations">Partial Dependence Foundations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Management</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="data_management.html">Data Management Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_management.html#data-management-techniques">Data Management Techniques</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Plotting Heuristics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="eda_plots.html">Creating Effective Visualizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="eda_plots.html#kde-and-histogram-distribution-plots">KDE and Histogram Distribution Plots</a></li>
<li class="toctree-l1"><a class="reference internal" href="eda_plots.html#feature-scaling-and-outliers">Feature Scaling and Outliers</a></li>
<li class="toctree-l1"><a class="reference internal" href="eda_plots.html#stacked-crosstab-plots">Stacked Crosstab Plots</a></li>
<li class="toctree-l1"><a class="reference internal" href="eda_plots.html#box-and-violin-plots">Box and Violin Plots</a></li>
<li class="toctree-l1"><a class="reference internal" href="eda_plots.html#scatter-plots-and-best-fit-lines">Scatter Plots and Best Fit Lines</a></li>
<li class="toctree-l1"><a class="reference internal" href="eda_plots.html#correlation-matrices">Correlation Matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="eda_plots.html#partial-dependence-plots">Partial Dependence Plots</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About EDA Toolkit</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="art.html">ASCII Art</a></li>
<li class="toctree-l1"><a class="reference internal" href="acknowledgements.html">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributors.html">Contributors/Maintainers</a></li>
<li class="toctree-l1"><a class="reference internal" href="citations.html">Citing EDA Toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">EDA Toolkit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Gaussian Assumption for Normality</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="no-click"><a class="reference internal image-reference" href="_images/eda_toolkit_logo.svg"><img alt="EDA Toolkit Logo" class="align-left" src="_images/eda_toolkit_logo.svg" width="300px" /></a>
</div><div style="height: 100px;"></div><p></p>
<section id="gaussian-assumption-for-normality">
<h1>Gaussian Assumption for Normality<a class="headerlink" href="#gaussian-assumption-for-normality" title="Permalink to this heading"></a></h1>
<p>The Gaussian (normal) distribution is a key assumption in many statistical methods. It is mathematically represented by the probability density function (PDF):</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu\)</span> is the mean</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma^2\)</span> is the variance</p></li>
</ul>
<p>In a normally distributed dataset:</p>
<ul class="simple">
<li><p>68% of data falls within <span class="math notranslate nohighlight">\(\mu \pm \sigma\)</span></p></li>
<li><p>95% within <span class="math notranslate nohighlight">\(\mu \pm 2\sigma\)</span></p></li>
<li><p>99.7% within <span class="math notranslate nohighlight">\(\mu \pm 3\sigma\)</span></p></li>
</ul>
<div class="no-click"><a class="reference internal image-reference" href="_images/normal_distribution.png"><img alt="KDE Distributions - KDE (+) Histograms (Density)" class="align-center" src="_images/normal_distribution.png" style="width: 950px;" /></a>
</div><div style="height: 50px;"></div><section id="histograms-and-kernel-density-estimation-kde">
<h2>Histograms and Kernel Density Estimation (KDE)<a class="headerlink" href="#histograms-and-kernel-density-estimation-kde" title="Permalink to this heading"></a></h2>
<p><strong>Histograms</strong>:</p>
<ul class="simple">
<li><p>Visualize data distribution by binning values and counting frequencies.</p></li>
<li><p>If data is Gaussian, the histogram approximates a bell curve.</p></li>
</ul>
<p><strong>KDE</strong>:</p>
<ul class="simple">
<li><p>A non-parametric way to estimate the PDF by smoothing individual data points with a kernel function.</p></li>
<li><p>The KDE for a dataset <span class="math notranslate nohighlight">\(X = \{x_1, x_2, \ldots, x_n\}\)</span> is given by:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(K\)</span> is the kernel function (often Gaussian)</p></li>
<li><p><span class="math notranslate nohighlight">\(h\)</span> is the bandwidth (smoothing parameter)</p></li>
</ul>
<b><a href="../eda_plots.html#kde_hist_plots">Combined Use of Histograms and KDE</a></b><p></p>
<ul class="simple">
<li><p><strong>Histograms</strong> offer a discrete, binned view of the data.</p></li>
<li><p><strong>KDE</strong> provides a smooth, continuous estimate of the underlying distribution.</p></li>
<li><p>Together, they effectively illustrate how well the data aligns with the Gaussian assumption, highlighting any deviations from normality.</p></li>
</ul>
</section>
</section>
<section id="pearson-correlation-coefficient">
<h1>Pearson Correlation Coefficient<a class="headerlink" href="#pearson-correlation-coefficient" title="Permalink to this heading"></a></h1>
<p>The Pearson correlation coefficient, often denoted as <span class="math notranslate nohighlight">\(r\)</span>, is a measure of
the linear relationship between two variables. It quantifies the degree to which
a change in one variable is associated with a change in another variable. The
Pearson correlation ranges from <span class="math notranslate nohighlight">\(-1\)</span> to <span class="math notranslate nohighlight">\(1\)</span>, where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(r = 1\)</span> indicates a perfect positive linear relationship.</p></li>
<li><p><span class="math notranslate nohighlight">\(r = -1\)</span> indicates a perfect negative linear relationship.</p></li>
<li><p><span class="math notranslate nohighlight">\(r = 0\)</span> indicates no linear relationship.</p></li>
</ul>
<p>The Pearson correlation coefficient between two variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[r_{XY} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(X, Y)\)</span> is the covariance of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_X\)</span> is the standard deviation of <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_Y\)</span> is the standard deviation of <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
</ul>
<p>Covariance measures how much two variables change together. It is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (X_i - \mu_X)(Y_i - \mu_Y)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of data points.</p></li>
<li><p><span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(Y_i\)</span> are the individual data points.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu_X\)</span> and <span class="math notranslate nohighlight">\(\mu_Y\)</span> are the means of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
</ul>
<p>The standard deviation measures the dispersion or spread of a set of values. For
a variable <span class="math notranslate nohighlight">\(X\)</span>, the standard deviation <span class="math notranslate nohighlight">\(\sigma_X\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\sigma_X = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (X_i - \mu_X)^2}\]</div>
<p>Substituting the covariance and standard deviation into the Pearson correlation formula:</p>
<div class="math notranslate nohighlight">
\[r_{XY} = \frac{\sum_{i=1}^{n} (X_i - \mu_X)(Y_i - \mu_Y)}{\sqrt{\sum_{i=1}^{n} (X_i - \mu_X)^2} \sqrt{\sum_{i=1}^{n} (Y_i - \mu_Y)^2}}\]</div>
<p>This formula normalizes the covariance by the product of the standard deviations of the two variables, resulting in a dimensionless coefficient that indicates the strength and direction of the linear relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(r &gt; 0\)</span>: Positive correlation. As <span class="math notranslate nohighlight">\(X\)</span> increases, <span class="math notranslate nohighlight">\(Y\)</span> tends to increase.</p></li>
<li><p><span class="math notranslate nohighlight">\(r &lt; 0\)</span>: Negative correlation. As <span class="math notranslate nohighlight">\(X\)</span> increases, <span class="math notranslate nohighlight">\(Y\)</span> tends to decrease.</p></li>
<li><p><span class="math notranslate nohighlight">\(r = 0\)</span>: No linear correlation. There is no consistent linear relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
</ul>
<p>The closer the value of <span class="math notranslate nohighlight">\(r\)</span> is to <span class="math notranslate nohighlight">\(\pm 1\)</span>, the stronger the linear relationship between the two variables.</p>
</section>
<section id="box-cox-transformation">
<span id="id1"></span><h1>Box-Cox Transformation<a class="headerlink" href="#box-cox-transformation" title="Permalink to this heading"></a></h1>
<p>The Box-Cox transformation is a powerful technique for stabilizing variance and
making data more closely follow a normal distribution. Developed by statisticians
George Box and David Cox in 1964, the transformation is particularly useful in
linear regression models where assumptions of normality and homoscedasticity are
necessary. This document provides an accessible overview of the theoretical
concepts underlying the Box-Cox transformation.</p>
<p>Many statistical methods assume that data is normally distributed and that the
variance remains constant across observations (homoscedasticity). However,
real-world data often violates these assumptions, especially when dealing with
positive-only, skewed distributions (e.g., income, expenditure, biological measurements).
The Box-Cox transformation is a family of power transformations designed to address
these issues by “normalizing” the data and stabilizing variance.</p>
<section id="mathematical-definition">
<h2>Mathematical Definition<a class="headerlink" href="#mathematical-definition" title="Permalink to this heading"></a></h2>
<p>The Box-Cox transformation is defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}y(\lambda) =
\begin{cases}
  \frac{y^{\lambda} - 1}{\lambda}, &amp; \text{if } \lambda \neq 0 \\
  \ln(y), &amp; \text{if } \lambda = 0
\end{cases}\end{split}\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y(\lambda)\)</span> is the transformed variable,</p></li>
<li><p><span class="math notranslate nohighlight">\(y\)</span> is the original variable (positive and continuous),</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> is the transformation parameter.</p></li>
</ul>
<p>When <span class="math notranslate nohighlight">\(\lambda = 0\)</span>, the transformation becomes a natural logarithm, effectively a special case of the Box-Cox transformation.</p>
<p><strong>Interpretation of the Lambda Parameter</strong></p>
<p>The value of <span class="math notranslate nohighlight">\(\lambda\)</span> determines the shape of the transformation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda = 1\)</span>: The transformation does nothing; the data remains unchanged.</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda = 0.5\)</span>: A square-root transformation.</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda = 0\)</span>: A logarithmic transformation.</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda &lt; 0\)</span>: An inverse transformation, which is often helpful when working with highly skewed data.</p></li>
</ul>
<p>Selecting the optimal value of <span class="math notranslate nohighlight">\(\lambda\)</span> to achieve approximate normality or homoscedasticity is typically done using maximum likelihood estimation (MLE), where the goal is to find the value of <span class="math notranslate nohighlight">\(\lambda\)</span> that maximizes the likelihood of observing the transformed data under a normal distribution.</p>
</section>
<section id="properties-and-benefits">
<h2>Properties and Benefits<a class="headerlink" href="#properties-and-benefits" title="Permalink to this heading"></a></h2>
<p>The Box-Cox transformation has two key properties:</p>
<ol class="arabic simple">
<li><p><strong>Variance Stabilization</strong>: By choosing an appropriate <span class="math notranslate nohighlight">\(\lambda\)</span>, the variance of <span class="math notranslate nohighlight">\(y(\lambda)\)</span> can be made more constant across levels of <span class="math notranslate nohighlight">\(y\)</span>. This is particularly useful in regression analysis, as homoscedasticity is often a critical assumption.</p></li>
<li><p><strong>Normalization</strong>: The transformation makes the distribution of <span class="math notranslate nohighlight">\(y(\lambda)\)</span> closer to normality. This allows statistical techniques that assume normality to be more applicable to real-world, skewed data.</p></li>
</ol>
<p><strong>Likelihood Function</strong></p>
<p>The likelihood function for the Box-Cox transformation is derived from the assumption that the transformed data follows a normal distribution. For a dataset with observations <span class="math notranslate nohighlight">\(y_i\)</span>, the likelihood function is given by:</p>
<div class="math notranslate nohighlight">
\[L(\lambda) = -\frac{n}{2} \ln (s^2) + (\lambda - 1) \sum_{i=1}^{n} \ln(y_i),\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of observations,</p></li>
<li><p><span class="math notranslate nohighlight">\(s^2\)</span> is the sample variance of the transformed data.</p></li>
</ul>
<p>Maximizing this likelihood function provides the MLE for <span class="math notranslate nohighlight">\(\lambda\)</span>, which can be estimated using iterative methods.</p>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Permalink to this heading"></a></h2>
<p>In practice, implementing the Box-Cox transformation requires a few considerations:</p>
<ul class="simple">
<li><p><strong>Positive-Only Data</strong>: The transformation is only defined for positive values. For datasets with zero or negative values, a constant can be added to make all observations positive before applying the transformation.</p></li>
<li><p><strong>Interpretability</strong>: The transformed data may lose interpretability in its original scale. For some applications, this trade-off is justified to meet model assumptions.</p></li>
<li><p><strong>Inverse Transformation</strong>: If interpretability is a concern, the inverse of the Box-Cox transformation can be applied to transform results back to the original scale.</p></li>
</ul>
</section>
<section id="applications-in-modeling">
<h2>Applications in Modeling<a class="headerlink" href="#applications-in-modeling" title="Permalink to this heading"></a></h2>
<p>In regression modeling, the Box-Cox transformation can improve both the accuracy
and validity of predictions. For example, in Ordinary Least Squares (OLS)
regression, the transformation reduces heteroscedasticity and normalizes residuals,
leading to more reliable parameter estimates. Similarly, in time series analysis,
the Box-Cox transformation can stabilize variance, making models such as ARIMA more effective.</p>
<p>The Box-Cox transformation is a flexible and powerful technique for addressing
non-normality and heteroscedasticity in data. By choosing an appropriate <span class="math notranslate nohighlight">\(\lambda\)</span>,
practitioners can transform data to better meet the assumptions of various statistical methods,
enhancing the reliability of their models and inferences.</p>
<section id="confidence-intervals-for-lambda">
<span id="id2"></span><h3>Confidence Intervals for Lambda<a class="headerlink" href="#confidence-intervals-for-lambda" title="Permalink to this heading"></a></h3>
<p>In practice, it is often helpful to assess the stability of the estimated
transformation parameter <span class="math notranslate nohighlight">\(\lambda\)</span> by constructing a confidence interval
(CI). The CI provides a range of values within which the true value of <span class="math notranslate nohighlight">\(\lambda\)</span>
is likely to fall, offering insights into the sensitivity of the transformation.</p>
<p>To construct a confidence interval for <span class="math notranslate nohighlight">\(\lambda\)</span>, the following approach can be used:</p>
<p>1. <strong>Alpha Level</strong>: Select an alpha level, commonly 0.05, for a 95% confidence
interval, or adjust as needed. The alpha level represents the probability of
observing a value outside this interval if the estimate were repeated multiple times.</p>
<p>2. <strong>Profile Likelihood Method</strong>: One approach is to use the profile likelihood
method, where a range of <span class="math notranslate nohighlight">\(\lambda\)</span> values are tested, and those with
likelihoods close to the maximum likelihood estimate (MLE) are retained within
the interval. The confidence interval is defined as the set of <span class="math notranslate nohighlight">\(\lambda\)</span>
values for which the likelihood ratio statistic:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{LR}(\lambda) = 2 \left( L(\hat{\lambda}) - L(\lambda) \right)\]</div>
<p>is less than the chi-square value at the chosen confidence level (e.g., 3.84 for a 95% CI with one degree of freedom).</p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Interpretation</strong>: A narrow CI around <span class="math notranslate nohighlight">\(\lambda\)</span> suggests that the transformation is relatively stable, while a wide interval might indicate sensitivity, signaling that the data may benefit from an alternative transformation or modeling approach.</p></li>
</ol>
<p>These confidence intervals provide a more robust understanding of the transformation’s impact, as well as the degree of transformation needed to meet model assumptions.</p>
</section>
</section>
</section>
<section id="the-yeo-johnson-transformation">
<h1>The Yeo-Johnson Transformation<a class="headerlink" href="#the-yeo-johnson-transformation" title="Permalink to this heading"></a></h1>
<p>For a feature <span class="math notranslate nohighlight">\(y\)</span>, the Yeo-Johnson transformation <span class="math notranslate nohighlight">\(Y\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}Y =
\begin{cases}
\frac{((y + 1)^{\lambda} - 1)}{\lambda} &amp; \text{if } y \geq 0, \lambda \neq 0 \\
\ln(y + 1) &amp; \text{if } y \geq 0, \lambda = 0 \\
-\frac{((-y + 1)^{2 - \lambda} - 1)}{2 - \lambda} &amp; \text{if } y &lt; 0, \lambda \neq 2 \\
-\ln(-y + 1) &amp; \text{if } y &lt; 0, \lambda = 2
\end{cases}\end{split}\]</div>
<p><strong>Breakdown of the Conditions</strong></p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>For Positive Values of <span class="math notranslate nohighlight">\(y\)</span> (<span class="math notranslate nohighlight">\(y \geq 0\)</span>):</dt><dd><ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(\lambda \neq 0\)</span>: The transformation behaves similarly to the Box-Cox transformation with <span class="math notranslate nohighlight">\((y + 1)^{\lambda}\)</span>.</p></li>
<li><p>When <span class="math notranslate nohighlight">\(\lambda = 0\)</span>: The transformation simplifies to the natural log, <span class="math notranslate nohighlight">\(\ln(y + 1)\)</span>.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>For Negative Values of <span class="math notranslate nohighlight">\(y\)</span> (<span class="math notranslate nohighlight">\(y &lt; 0\)</span>):</dt><dd><ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(\lambda \neq 2\)</span>: A reflected transformation is applied, <span class="math notranslate nohighlight">\(-(−y + 1)^{2 - \lambda}\)</span>, to manage negative values smoothly.</p></li>
<li><p>When <span class="math notranslate nohighlight">\(\lambda = 2\)</span>: The transformation simplifies to <span class="math notranslate nohighlight">\(- \ln(-y + 1)\)</span>, making it suitable for negative inputs while preserving continuity.</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
<p><strong>Why It Works</strong></p>
<p>The Yeo-Johnson transformation adjusts data to make it more normally distributed. By allowing transformations for both positive and negative values, it offers flexibility across various distributions. The parameter <span class="math notranslate nohighlight">\(\lambda\)</span> is typically optimized to best approximate normality.</p>
<p><strong>When to Use It</strong></p>
<p>Yeo-Johnson is particularly useful for datasets containing zero or negative values. It’s often effective for linear models that assume normally distributed data, making it a robust alternative when Box-Cox cannot be applied.</p>
</section>
<section id="median-and-iqr-scaling">
<span id="robust-scaler"></span><h1>Median and IQR Scaling<a class="headerlink" href="#median-and-iqr-scaling" title="Permalink to this heading"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">RobustScaler</span></code> in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> is a scaling method that reduces the impact
of outliers in your data by using the <strong>median</strong> and <strong>interquartile range (IQR)</strong>
instead of the mean and standard deviation, which are more sensitive to extreme values.
Here’s a mathematical breakdown of how it works:</p>
<section id="centering-data-using-the-median">
<h2>Centering Data Using the Median<a class="headerlink" href="#centering-data-using-the-median" title="Permalink to this heading"></a></h2>
<p>The formula for scaling each feature <span class="math notranslate nohighlight">\(x\)</span> in the dataset using <code class="docutils literal notranslate"><span class="pre">RobustScaler</span></code> is:</p>
<div class="math notranslate nohighlight">
\[x_{\text{scaled}} = \frac{x - \text{Median}(x)}{\text{IQR}(x)}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{Median}(x)\)</span> is the median of the feature <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{IQR}(x) = Q_3 - Q_1\)</span>, the interquartile range, is the difference between the 75th percentile (<span class="math notranslate nohighlight">\(Q_3\)</span>) and the 25th percentile (<span class="math notranslate nohighlight">\(Q_1\)</span>) of the feature. This range represents the spread of the middle 50% of values, which is less sensitive to extreme values than the total range.</p></li>
</ul>
</section>
<section id="explanation-of-each-component">
<h2>Explanation of Each Component<a class="headerlink" href="#explanation-of-each-component" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Median</strong> (<span class="math notranslate nohighlight">\(\text{Median}(x)\)</span>): This is the 50th percentile, or the central value of the feature. It acts as the “center” of the data, but unlike the mean, it is robust to outliers.</p></li>
<li><p><strong>Interquartile Range (IQR)</strong>: By dividing by the IQR, the <code class="docutils literal notranslate"><span class="pre">RobustScaler</span></code> standardizes the spread of the data based on the range of the middle 50% of values, making it less influenced by extreme values. Essentially, the values are scaled to fall within a range close to -1 to 1 for the majority of samples.</p></li>
</ul>
</section>
<section id="example-calculation">
<h2>Example Calculation<a class="headerlink" href="#example-calculation" title="Permalink to this heading"></a></h2>
<p>Suppose you have a feature <span class="math notranslate nohighlight">\(x = [1, 2, 3, 4, 5, 100]\)</span>. Here’s how the scaling would work:</p>
<ol class="arabic">
<li><p><strong>Calculate the Median</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{Median}(x) = 3.5\]</div>
</li>
<li><p><strong>Calculate the Interquartile Range (IQR)</strong>:</p>
<ul class="simple">
<li><p>First, find <span class="math notranslate nohighlight">\(Q_1\)</span> (25th percentile) and <span class="math notranslate nohighlight">\(Q_3\)</span> (75th percentile):
- <span class="math notranslate nohighlight">\(Q_1 = 2\)</span>, <span class="math notranslate nohighlight">\(Q_3 = 5\)</span></p></li>
<li><p>Then, <span class="math notranslate nohighlight">\(\text{IQR}(x) = Q_3 - Q_1 = 5 - 2 = 3\)</span></p></li>
</ul>
</li>
</ol>
<p></p>
<ol class="arabic" start="3">
<li><p><strong>Apply the Scaling Formula</strong>:</p>
<ul class="simple">
<li><p>For each <span class="math notranslate nohighlight">\(x\)</span> value, subtract the median and divide by the IQR:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[x_{\text{scaled}} = \frac{x - 3.5}{3}\]</div>
</li>
</ol>
<p>This results in values that are centered around 0 and scaled according to the
interquartile range, rather than the full range or mean and standard deviation.
For our example, the outlier (100) will be downscaled effectively, reducing its
influence on the data’s range and making the scaling robust to such outliers.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">RobustScaler</span></code> is particularly useful when dealing with data with significant
outliers, as it centers the data around the median and scales according to the
IQR, allowing for better handling of extreme values than traditional
standardization methods.</p>
</section>
</section>
<section id="logit-transformation">
<span id="logit-assumptions"></span><h1>Logit Transformation<a class="headerlink" href="#logit-transformation" title="Permalink to this heading"></a></h1>
<p>The logit transformation is used to map values from the range <span class="math notranslate nohighlight">\((0, 1)\)</span> to the entire real number line <span class="math notranslate nohighlight">\((-\infty, +\infty)\)</span>.
It is defined mathematically as:</p>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \ln\left(\frac{p}{1 - p}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is a value in the range <span class="math notranslate nohighlight">\((0, 1)\)</span>. In other words, for each value <span class="math notranslate nohighlight">\(p\)</span>, the transformation is calculated
by taking the natural logarithm of the odds <span class="math notranslate nohighlight">\(p / (1 - p)\)</span>.</p>
<section id="purpose-and-assumptions">
<h2>Purpose and Assumptions<a class="headerlink" href="#purpose-and-assumptions" title="Permalink to this heading"></a></h2>
<p>The logit function is particularly useful in scenarios where data is constrained between 0 and 1, such as probabilities
or proportions. However, to apply this transformation, <strong>all values must strictly lie within the open interval</strong> <span class="math notranslate nohighlight">\((0, 1)\)</span>.
Values equal to 0 or 1 result in undefined values <span class="math notranslate nohighlight">\((-\infty, +\infty\)</span> respectively) since the logarithm of zero is undefined.</p>
<p>In the code implementation, a <code class="docutils literal notranslate"><span class="pre">ValueError</span></code> is raised if any values in the target feature fall outside the
interval <span class="math notranslate nohighlight">\((0, 1)\)</span>. If your data does not meet this condition, consider applying a <strong>Min-Max scaling</strong> first to transform
the data to the appropriate range.</p>
<p><strong>Example</strong></p>
<p>If <span class="math notranslate nohighlight">\(p = 0.5\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[\text{logit}(0.5) = \ln\left(\frac{0.5}{1 - 0.5}\right) = \ln(1) = 0\]</div>
</section>
</section>
<section id="partial-dependence-foundations">
<h1>Partial Dependence Foundations<a class="headerlink" href="#partial-dependence-foundations" title="Permalink to this heading"></a></h1>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> represent the complete set of input features for a machine
learning model, where <span class="math notranslate nohighlight">\(\mathbf{X} = \{X_1, X_2, \dots, X_p\}\)</span>. Suppose we’re
particularly interested in a subset of these features, denoted by <span class="math notranslate nohighlight">\(\mathbf{X}_S\)</span>.
The complementary set, <span class="math notranslate nohighlight">\(\mathbf{X}_C\)</span>, contains all the features in <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>
that are not in <span class="math notranslate nohighlight">\(\mathbf{X}_S\)</span>. Mathematically, this relationship is expressed as:</p>
<div class="math notranslate nohighlight">
\[\mathbf{X}_C = \mathbf{X} \setminus \mathbf{X}_S\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{X}_C\)</span> is the set of features in <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> after
removing the features in <span class="math notranslate nohighlight">\(\mathbf{X}_S\)</span>.</p>
<p>Partial Dependence Plots (PDPs) are used to illustrate the effect of the features
in <span class="math notranslate nohighlight">\(\mathbf{X}_S\)</span> on the model’s predictions, while averaging out the
influence of the features in <span class="math notranslate nohighlight">\(\mathbf{X}_C\)</span>. This is mathematically defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\text{PD}_{\mathbf{X}_S}(x_S) &amp;= \mathbb{E}_{\mathbf{X}_C} \left[ f(x_S, \mathbf{X}_C) \right] \\
&amp;= \int f(x_S, x_C) \, p(x_C) \, dx_C \\
&amp;= \int \left( \int f(x_S, x_C) \, p(x_C \mid x_S) \, dx_C \right) p(x_S) \, dx_S
\end{align*}\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_{\mathbf{X}_C} \left[ \cdot \right]\)</span> indicates that we are taking the expected value over the possible values of the features in the set <span class="math notranslate nohighlight">\(\mathbf{X}_C\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(p(x_C)\)</span> represents the probability density function of the features in <span class="math notranslate nohighlight">\(\mathbf{X}_C\)</span>.</p></li>
</ul>
<p>This operation effectively summarizes the model’s output over all potential values of the complementary features, providing a clear view of how the features in <span class="math notranslate nohighlight">\(\mathbf{X}_S\)</span> alone impact the model’s predictions.</p>
<p><strong>2D Partial Dependence Plots</strong></p>
<p>Consider a trained machine learning model <a class="reference internal" href="eda_plots.html#d-partial-dependence-plots"><span class="std std-ref">2D Partial Dependence Plots</span></a><span class="math notranslate nohighlight">\(f(\mathbf{X})\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{X} = (X_1, X_2, \dots, X_p)\)</span> represents the vector of input features. The partial dependence of the predicted response <span class="math notranslate nohighlight">\(\hat{y}\)</span> on a single feature <span class="math notranslate nohighlight">\(X_j\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{PD}(X_j) = \frac{1}{n} \sum_{i=1}^{n} f(X_j, \mathbf{X}_{C_i})\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X_j\)</span> is the feature of interest.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}_{C_i}\)</span> represents the complement set of <span class="math notranslate nohighlight">\(X_j\)</span>, meaning the remaining features in <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> not included in <span class="math notranslate nohighlight">\(X_j\)</span> for the <span class="math notranslate nohighlight">\(i\)</span>-th instance.</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of observations in the dataset.</p></li>
</ul>
<p>For two features, <span class="math notranslate nohighlight">\(X_j\)</span> and <span class="math notranslate nohighlight">\(X_k\)</span>, the partial dependence is given by:</p>
<div class="math notranslate nohighlight">
\[\text{PD}(X_j, X_k) = \frac{1}{n} \sum_{i=1}^{n} f(X_j, X_k, \mathbf{X}_{C_i})\]</div>
<p>This results in a 2D surface plot (or contour plot) that shows how the predicted outcome changes as the values of <span class="math notranslate nohighlight">\(X_j\)</span> and <span class="math notranslate nohighlight">\(X_k\)</span> vary, while the effects of the other features are averaged out.</p>
<ul class="simple">
<li><p><strong>Single Feature PDP:</strong> When plotting <span class="math notranslate nohighlight">\(\text{PD}(X_j)\)</span>, the result is a 2D line plot showing the marginal effect of feature <span class="math notranslate nohighlight">\(X_j\)</span> on the predicted outcome, averaged over all possible values of the other features.</p></li>
<li><p><strong>Two Features PDP:</strong> When plotting <span class="math notranslate nohighlight">\(\text{PD}(X_j, X_k)\)</span>, the result is a 3D surface plot (or a contour plot) that shows the combined marginal effect of <span class="math notranslate nohighlight">\(X_j\)</span> and <span class="math notranslate nohighlight">\(X_k\)</span> on the predicted outcome. The surface represents the expected value of the prediction as <span class="math notranslate nohighlight">\(X_j\)</span> and <span class="math notranslate nohighlight">\(X_k\)</span> vary, while all other features are averaged out.</p></li>
</ul>
<p><strong>3D Partial Dependence Plots</strong></p>
<p>For a more comprehensive analysis, especially when exploring interactions between two features, <a class="reference internal" href="eda_plots.html#id19"><span class="std std-ref">3D Partial Dependence Plots</span></a> are invaluable. The partial dependence function for two features in a 3D context is:</p>
<div class="math notranslate nohighlight">
\[\text{PD}(X_j, X_k) = \frac{1}{n} \sum_{i=1}^{n} f(X_j, X_k, \mathbf{X}_{C_i})\]</div>
<p>Here, the function <span class="math notranslate nohighlight">\(f(X_j, X_k, \mathbf{X}_{C_i})\)</span> is evaluated across a grid of values for <span class="math notranslate nohighlight">\(X_j\)</span> and <span class="math notranslate nohighlight">\(X_k\)</span>. The resulting 3D surface plot represents how the model’s prediction changes over the joint range of these two features.</p>
<p>The 3D plot offers a more intuitive visualization of feature interactions compared to 2D contour plots, allowing for a better understanding of the combined effects of features on the model’s predictions. The surface plot is particularly useful when you need to capture complex relationships that might not be apparent in 2D.</p>
<ul class="simple">
<li><p><strong>Feature Interaction Visualization:</strong> The 3D PDP provides a comprehensive view of the interaction between two features. The resulting surface plot allows for the visualization of how the model’s output changes when the values of two features are varied simultaneously, making it easier to understand complex interactions.</p></li>
<li><p><strong>Enhanced Interpretation:</strong> 3D PDPs offer enhanced interpretability in scenarios where feature interactions are not linear or where the effect of one feature depends on the value of another. The 3D visualization makes these dependencies more apparent.</p></li>
</ul>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="getting_started.html" class="btn btn-neutral float-left" title="Welcome to the EDA Toolkit Python Library Documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data_management.html" class="btn btn-neutral float-right" title="Data Management Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Leonid Shpaner, Oscar Gil.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>